{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17635,
     "status": "ok",
     "timestamp": 1675413392495,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "rclSvuvrDE-2",
    "outputId": "2ef71a5a-b90d-4538-a3cc-3d369a16b5e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxubCtIHlHfQ"
   },
   "source": [
    "## Package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13690,
     "status": "ok",
     "timestamp": 1675413406182,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "_lMlDo7ElMrU",
    "outputId": "7eb5efa6-fe51-44dd-92ef-d9530a613589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (57.4.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-67.1.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 57.4.0\n",
      "    Uninstalling setuptools-57.4.0:\n",
      "      Successfully uninstalled setuptools-57.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
      "cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed setuptools-67.1.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-0.21.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-0.21.1\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade setuptools\n",
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 102289,
     "status": "ok",
     "timestamp": 1675413508467,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "mF6-j13jCyUz",
    "outputId": "df2c9234-5a6e-4fb9-bf79-e4abb47cc4d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: opencv-python 4.6.0.66\n",
      "Uninstalling opencv-python-4.6.0.66:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.8/dist-packages/cv2/*\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python-4.6.0.66.dist-info/*\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libQt5Core-39545cc7.so.5.15.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libQt5Gui-48e93776.so.5.15.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libQt5Test-c38a5234.so.5.15.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libQt5Widgets-e69d94fb.so.5.15.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libQt5XcbQpa-c112ba75.so.5.15.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libX11-xcb-69166bdf.so.1.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libXau-00ec42fe.so.6.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libavcodec-5896f664.so.58.134.100\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libavformat-8ef5c7db.so.58.76.100\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libavutil-9c768859.so.56.70.100\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libbz2-a273e504.so.1.0.6\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libcrypto-d21001fc.so.1.1\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libgfortran-91cc3cb1.so.3.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libopenblas-r0-f650aae0.3.3.so\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libpng16-57e5e0a0.so.16.37.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libquadmath-96973f99.so.0.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libssl-c8c53640.so.1.1\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libswresample-99364a1c.so.3.9.100\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libswscale-e6451464.so.5.9.100\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libvpx-f22f1483.so.7.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-icccm-413c9f41.so.4.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-image-e82a276d.so.0.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-keysyms-21015570.so.1.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-randr-a96a5a87.so.0.1.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-render-637b984a.so.0.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-render-util-43ce00f5.so.0.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-shape-25c2b258.so.0.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-shm-7a199f70.so.0.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-sync-89374f40.so.1.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-util-4d666913.so.1.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-xfixes-9be3ba6f.so.0.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-xinerama-ae147f87.so.0.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxcb-xkb-9ba31ab3.so.1.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxkbcommon-71ae2972.so.0.0.0\n",
      "    /usr/local/lib/python3.8/dist-packages/opencv_python.libs/libxkbcommon-x11-c65ed502.so.0.0.0\n",
      "  Would not remove (might be manually added):\n",
      "    /usr/local/lib/python3.8/dist-packages/cv2/data/haarcascade_license_plate_rus_16stages.xml\n",
      "Proceed (Y/n)? Y\n",
      "  Successfully uninstalled opencv-python-4.6.0.66\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.21.6)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.7.0.68\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting geopandas\n",
      "  Downloading geopandas-0.12.2-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyproj>=2.6.1.post1\n",
      "  Downloading pyproj-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from geopandas) (1.3.5)\n",
      "Collecting fiona>=1.8\n",
      "  Downloading Fiona-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: shapely>=1.7 in /usr/local/lib/python3.8/dist-packages (from geopandas) (2.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from geopandas) (23.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from fiona>=1.8->geopandas) (22.2.0)\n",
      "Collecting click-plugins>=1.0\n",
      "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting click~=8.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cligj>=0.5\n",
      "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Collecting munch>=2.3.2\n",
      "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from fiona>=1.8->geopandas) (2022.12.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->geopandas) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->geopandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0.0->geopandas) (1.21.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from munch>=2.3.2->fiona>=1.8->geopandas) (1.15.0)\n",
      "Installing collected packages: pyproj, munch, click, cligj, click-plugins, fiona, geopandas\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 7.1.2\n",
      "    Uninstalling click-7.1.2:\n",
      "      Successfully uninstalled click-7.1.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed click-8.1.3 click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.0 geopandas-0.12.2 munch-2.5.0 pyproj-3.4.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting rtree\n",
      "  Downloading Rtree-1.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rtree\n",
      "Successfully installed rtree-1.0.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting fiftyone\n",
      "  Downloading fiftyone-0.18.0-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from fiftyone) (3.2.2)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from fiftyone) (0.16.0)\n",
      "Collecting voxel51-eta<0.9,>=0.8.1\n",
      "  Downloading voxel51_eta-0.8.1-py2.py3-none-any.whl (564 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.0/564.0 KB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from fiftyone) (1.0.2)\n",
      "Collecting fiftyone-brain<0.10,>=0.9.2\n",
      "  Downloading fiftyone_brain-0.9.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pymongo>=3.11 in /usr/local/lib/python3.8/dist-packages (from fiftyone) (4.3.3)\n",
      "Collecting pprintpp\n",
      "  Downloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from fiftyone) (0.18.3)\n",
      "Collecting motor>=2.3\n",
      "  Downloading motor-3.1.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.2/56.2 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting eventlet\n",
      "  Downloading eventlet-0.33.3-py2.py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting strawberry-graphql==0.138.1\n",
      "  Downloading strawberry_graphql-0.138.1-py3-none-any.whl (192 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 KB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting argcomplete\n",
      "  Downloading argcomplete-2.0.0-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fiftyone) (1.3.5)\n",
      "Collecting Jinja2>=3\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.1/133.1 KB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ndjson\n",
      "  Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
      "Collecting mongoengine==0.24.2\n",
      "  Downloading mongoengine-0.24.2-py3-none-any.whl (108 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 KB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sse-starlette<1,>=0.10.3\n",
      "  Downloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n",
      "Collecting fiftyone-db<0.5,>=0.4\n",
      "  Downloading fiftyone_db-0.4.0-py3-none-manylinux1_x86_64.whl (37.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.8/37.8 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.26.63-py3-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hypercorn>=0.13.2\n",
      "  Downloading Hypercorn-0.14.3-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette==0.20.4\n",
      "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.8/dist-packages (from fiftyone) (4.7.0.68)\n",
      "Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.8/dist-packages (from fiftyone) (5.5.0)\n",
      "Collecting kaleido\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting retrying\n",
      "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from fiftyone) (67.1.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from fiftyone) (0.8.10)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from fiftyone) (2022.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fiftyone) (1.21.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from fiftyone) (23.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from fiftyone) (6.0)\n",
      "Collecting aiofiles\n",
      "  Downloading aiofiles-22.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from fiftyone) (5.4.8)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.8/dist-packages (from fiftyone) (7.1.2)\n",
      "Collecting xmltodict\n",
      "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
      "Collecting universal-analytics-python3<2,>=1.0.1\n",
      "  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting sseclient-py<2,>=1.7.2\n",
      "  Downloading sseclient_py-1.7.2-py2.py3-none-any.whl (8.4 kB)\n",
      "Collecting dacite>=1.6.0\n",
      "  Downloading dacite-1.8.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.20.4->fiftyone) (4.4.0)\n",
      "Collecting anyio<5,>=3.4.0\n",
      "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /usr/local/lib/python3.8/dist-packages (from strawberry-graphql==0.138.1->fiftyone) (2.8.2)\n",
      "Collecting graphql-core<3.3.0,>=3.2.0\n",
      "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from fiftyone-brain<0.10,>=0.9.2->fiftyone) (1.7.3)\n",
      "Collecting priority\n",
      "  Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n",
      "Collecting wsproto>=0.14.0\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting h2>=3.1.0\n",
      "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from hypercorn>=0.13.2->fiftyone) (0.10.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=3->fiftyone) (2.0.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly>=4.14->fiftyone) (8.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly>=4.14->fiftyone) (1.15.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.8/dist-packages (from pymongo>=3.11->fiftyone) (2.3.0)\n",
      "Collecting httpx>=0.10.0\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (2.4.0)\n",
      "Collecting patool\n",
      "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (1.24.3)\n",
      "Requirement already satisfied: glob2 in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (0.7)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (0.3.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (2.25.1)\n",
      "Requirement already satisfied: tzlocal in /usr/local/lib/python3.8/dist-packages (from voxel51-eta<0.9,>=0.8.1->fiftyone) (1.5.1)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting botocore<1.30.0,>=1.29.63\n",
      "  Downloading botocore-1.29.63-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from Deprecated->fiftyone) (1.14.1)\n",
      "Requirement already satisfied: greenlet>=0.3 in /usr/local/lib/python3.8/dist-packages (from eventlet->fiftyone) (2.0.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fiftyone) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fiftyone) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->fiftyone) (1.4.4)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->fiftyone) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->fiftyone) (2023.1.23.1)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->fiftyone) (2.9.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->fiftyone) (3.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fiftyone) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->fiftyone) (1.2.0)\n",
      "Collecting sniffio>=1.1\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.8/dist-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fiftyone) (2.10)\n",
      "Collecting urllib3\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hpack<5,>=4.0\n",
      "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
      "Collecting hyperframe<7,>=6.0\n",
      "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting httpcore<0.17.0,>=0.15.0\n",
      "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2022.12.7)\n",
      "Collecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->voxel51-eta<0.9,>=0.8.1->fiftyone) (4.0.0)\n",
      "Installing collected packages: sseclient-py, rfc3986, pprintpp, patool, ndjson, kaleido, xmltodict, urllib3, sniffio, retrying, priority, jmespath, Jinja2, hyperframe, hpack, h11, graphql-core, fiftyone-db, eventlet, Deprecated, dacite, argcomplete, aiofiles, wsproto, strawberry-graphql, motor, mongoengine, h2, botocore, anyio, starlette, s3transfer, hypercorn, httpcore, fiftyone-brain, voxel51-eta, sse-starlette, httpx, boto3, universal-analytics-python3, fiftyone\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "notebook 5.7.16 requires jinja2<=3.0.0, but you have jinja2 3.1.2 which is incompatible.\n",
      "flask 1.1.4 requires click<8.0,>=5.1, but you have click 8.1.3 which is incompatible.\n",
      "flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 Jinja2-3.1.2 aiofiles-22.1.0 anyio-3.6.2 argcomplete-2.0.0 boto3-1.26.63 botocore-1.29.63 dacite-1.8.0 eventlet-0.33.3 fiftyone-0.18.0 fiftyone-brain-0.9.2 fiftyone-db-0.4.0 graphql-core-3.2.3 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-0.16.3 httpx-0.23.3 hypercorn-0.14.3 hyperframe-6.0.1 jmespath-1.0.1 kaleido-0.2.1 mongoengine-0.24.2 motor-3.1.1 ndjson-0.3.1 patool-1.12 pprintpp-0.4.0 priority-2.0.0 retrying-1.3.4 rfc3986-1.5.0 s3transfer-0.6.0 sniffio-1.3.0 sse-starlette-0.10.3 sseclient-py-1.7.2 starlette-0.20.4 strawberry-graphql-0.138.1 universal-analytics-python3-1.1.1 urllib3-1.26.14 voxel51-eta-0.8.1 wsproto-1.2.0 xmltodict-0.13.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.8/dist-packages (0.21.1)\n"
     ]
    }
   ],
   "source": [
    "! pip uninstall opencv-python\n",
    "! pip install opencv-python\n",
    "! pip install geopandas\n",
    "! pip install rtree\n",
    "! pip install fiftyone\n",
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11088,
     "status": "ok",
     "timestamp": 1675413519550,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "jcDsAMW9DUBb",
    "outputId": "718a3530-dcb6-44a1-a255-9edff58320c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migrating database to v0.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fiftyone.migrations.runner:Migrating database to v0.18.0\n"
     ]
    }
   ],
   "source": [
    "global resolutions\n",
    "\n",
    "# precomputed resolutions of raster files by region\n",
    "resolutions = {\n",
    "        'arizona': 0.1521273311113449, # m/pixel\n",
    "        'sudan': 0.2941356391155026,\n",
    "        'china': 0.2789294695267606,\n",
    "        'rotorua': 0.1254966590856217,\n",
    "        'brazil': 0.4787769055150278,\n",
    "        'hartford': 0.0761820560981825,\n",
    "        'mexico': 0.1522336654793647,\n",
    "        'kansas': 0.1520310125331718,\n",
    "        'clyde': 0.1535755375821943,\n",
    "        'wilmington': 0.1522415121091316,\n",
    "        'dunedin': 0.1229411434163525,\n",
    "        'gisborne': 0.1253881937589445,\n",
    "        'palmertson': 0.1255661866324052,\n",
    "        'tauranga': 0.1254967811385344,\n",
    "        }\n",
    "\n",
    "import shutil\n",
    "import fiftyone as fo\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "from itertools import product\n",
    "from PIL import Image\n",
    "from shapely.geometry import Polygon\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5691,
     "status": "ok",
     "timestamp": 1675413525238,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "fCVJU3HpUZtD",
    "outputId": "b92c50f3-179b-47d7-d12b-894710b9eb13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting yacs\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from yacs) (6.0)\n",
      "Installing collected packages: yacs\n",
      "Successfully installed yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install yacs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1675413525239,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "oMAYifu6VY4I"
   },
   "outputs": [],
   "source": [
    "# basically a test if this is running in colab\n",
    "if 'content' in os.getcwd():\n",
    "    os.chdir('/content/drive/MyDrive/hertie_master_projects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2162,
     "status": "ok",
     "timestamp": 1675413527397,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "mfPkwUrjVc2R"
   },
   "outputs": [],
   "source": [
    "# importieren der functions aus den utils modules (Ordner ist im Drive hinzugefügt)\n",
    "from drive.MyDrive.hertie_master_projects.utils.dataset_utils import fix_annots, fix_filenames\n",
    "from drive.MyDrive.hertie_master_projects.utils.image_utils import downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1675413527398,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "ZwhBI9JtZ9gq",
    "outputId": "b6c9ddb6-c7d0-4886-a540-b5d7794942af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AZFrqkNaOof"
   },
   "outputs": [],
   "source": [
    "# false hier, weil er bei mir keinen .env file findet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1675413527398,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "yNWFkxk_dqvC"
   },
   "outputs": [],
   "source": [
    "# funktioniert nicht weil keine environment variablen gesetzt sind, habe aktuell keinen .env file (brauchen wir dann aufm Server dann wohl auch nicht mehr)\n",
    "\n",
    "#duke_path = os.environ.get('/content/drive/MyDrive/hertie_master_projects/raw_data/duke')\n",
    "#datasets_path = os.environ.get('/content/drive/MyDrive/hertie_master_projects/datasets')\n",
    "#root_path = os.environ.get('/content/drive/MyDrive/hertie_master_projects')\n",
    "#sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1675413527398,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "f4myXl8ramWb"
   },
   "outputs": [],
   "source": [
    "# daher habe ich jetzt hier die Pfade festgesetzt - müsste ggf angepasst werden je nach User (daher halt sonst mit .env)\n",
    "duke_path = \"/content/drive/MyDrive/hertie_master_projects/raw_data/duke\"\n",
    "datasets_path = \"/content/drive/MyDrive/hertie_master_projects/datasets\"\n",
    "root_path = \"/content/drive/MyDrive/hertie_master_projects\"\n",
    "sys.path.append(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1675413527399,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "8Di9YqWba0-W",
    "outputId": "3873ba47-48ca-46d1-c77a-eb31000869b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/hertie_master_projects/raw_data/duke\n"
     ]
    }
   ],
   "source": [
    "print(duke_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1675413527399,
     "user": {
      "displayName": "Johannes Halkenhäußer",
      "userId": "10212423884318197669"
     },
     "user_tz": -60
    },
    "id": "9VQsvmSRetX5"
   },
   "outputs": [],
   "source": [
    "#from src.utils.dataset_utils import fix_annots, fix_filenames (comment dinah: in oberer zelle für unser set-up angepasst)\n",
    "#from src.utils.image_utils import downsample (comment dinah: in oberer zelle für unser set-up angepasst)\n",
    "\n",
    "assert duke_path is not None, f'Could not locate .env file. Got duke_path {duke_path}'\n",
    "assert datasets_path is not None, f'Could not locate .env file. Got datasets_path {datasets_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQJ1MDhPfnAO"
   },
   "outputs": [],
   "source": [
    "def extract_duke_dataset(dirs, \n",
    "                         target_base_dir,\n",
    "                         size=512, \n",
    "                         base_path=\"\", \n",
    "                         train_ratio=0.8,\n",
    "                         target_resolution=None,\n",
    "                         bbox_threshold=None,\n",
    "                         tower_types=['DT', 'TT', 'OT'],\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Extracts training images and bounding boxes from region-zips provided in\n",
    "    'https://figshare.com/articles/dataset/Electric_Transmission_and_\n",
    "    Distribution_Infrastructure_Imagery_Dataset/6931088'\n",
    "    \n",
    "    Iterates over a list of directories and creates examples if it encounters the\n",
    "    following structure in these directories:\n",
    "        [dir_name]/raw/[.tif, .csv, .geojson etc. files]\n",
    "        stores images to the following structure\n",
    "        [dir_name]/examples/[prefix]+[id]+\".png\"]\n",
    "        and a summarizing geojson file storing a dataframe of filenames and bbox\n",
    "        [dir_name]/examples/[prefix]+\"examples.geojson\"\n",
    "    \n",
    "    Data must be unzipped!\n",
    "    Be sure that all directories in [dirs] are in os.getcwd()!\n",
    "    \n",
    "    ----------\n",
    "    Arguments:\n",
    "        dirs : (list of str)\n",
    "            list with names of directories which satisfy the outlined structure\n",
    "        target_base_dir (str):\n",
    "            directory where resulting datasets should be stored\n",
    "        size : (int)\n",
    "            width and height of resulting example-images (images are made quadratic)\n",
    "        base_path : (str)\n",
    "            path to directories from which all dirs are accessible\n",
    "        train_ratio : (float)\n",
    "            share of examples labelled as part of training set (rest is val set),\n",
    "        target_resolution: (None or float)\n",
    "            if float, resolutions are scaled to target_resolution\n",
    "        bbox_threshold(None or float):\n",
    "            if float, only towers are included that have bboxes which both for height and width are\n",
    "            height \n",
    "        tower_types(List[str]):\n",
    "            list of tower types that are taken into the dataset\n",
    "                TT: transmission towers\n",
    "                DT: distribution towers\n",
    "                OT: other towers\n",
    "        \n",
    "    ----------\n",
    "    Returns:\n",
    "        -\n",
    "    \"\"\"\n",
    "\n",
    "    prefixes = [word[:2].upper() for word in dirs]\n",
    "\n",
    "    width, height = size, size\n",
    "\n",
    "    if bbox_threshold is None:\n",
    "        bbox_threshold = 0.\n",
    "    \n",
    "    label = 'tower'\n",
    "\n",
    "    out_train = 'train'\n",
    "    out_val = 'val' \n",
    "\n",
    "    print('Starting dataset extraction')\n",
    "    print('Note currently all towers are labelled as tower')\n",
    "\n",
    "    if not os.path.isdir(target_base_dir): \n",
    "        os.mkdir(target_base_dir)\n",
    "\n",
    "    for country, prefix in zip(dirs, prefixes):\n",
    "        \n",
    "        # set up working path\n",
    "        print(\"Extracting images from {}...\".format(country))\n",
    "\n",
    "        print(f'Assuming resolution {resolutions[country]} m/pixel')\n",
    "        res = resolutions[country]\n",
    "\n",
    "        country_out = os.path.join(target_base_dir, country)\n",
    "        if not os.path.isdir(country_out): \n",
    "            os.mkdir(country_out)\n",
    "\n",
    "        train_path = os.path.join(country_out, out_train)\n",
    "        val_path = os.path.join(country_out, out_val)\n",
    "\n",
    "        if not os.path.isdir(train_path): \n",
    "            os.mkdir(train_path)\n",
    "        if not os.path.isdir(val_path): \n",
    "            os.mkdir(val_path)\n",
    "\n",
    "        # train_path = out_train\n",
    "\n",
    "        os.chdir(os.path.join(os.getcwd(), country, \"raw\"))\n",
    "        \n",
    "        # setup directory for resulting images\n",
    "        # set up resulting dataset of examples (with towers)\n",
    "        tower_df = gpd.GeoDataFrame({\"filename\": [], \n",
    "                                \"ul_x\": [], \"ul_y\": [], \"lr_x\": [], \"lr_y\": [], \n",
    "                                })\n",
    "        \n",
    "        # set up datasets for current country\n",
    "        try: \n",
    "            dataset_train = fo.Dataset(name=country+'_'+out_train)\n",
    "        except:\n",
    "            dataset_train = fo.load_dataset(country+'_'+out_train)\n",
    "            dataset_train.delete()\n",
    "            dataset_train = fo.Dataset(name=country+'_'+out_train)\n",
    "        dataset_train.persistent = False\n",
    "\n",
    "        try: \n",
    "            dataset_val = fo.Dataset(name=country+'_'+out_val)\n",
    "        except:\n",
    "            dataset_val = fo.load_dataset(country+'_'+out_val)\n",
    "            dataset_val.delete()\n",
    "            dataset_val = fo.Dataset(name=country+'_'+out_val)\n",
    "        dataset_val.persistent = False\n",
    "        \n",
    "        # Starting with adding examples to the training set\n",
    "        curr_path = train_path\n",
    "        curr_dataset = dataset_train\n",
    "        switched_already = False\n",
    "\n",
    "        # create list of relevant files\n",
    "        filelist = os.listdir()\n",
    "        csv_files = [fn for fn in filelist if fn.endswith('.csv')]\n",
    "\n",
    "        unders = [i for i, letter in enumerate(csv_files[0]) if letter is \"_\"]\n",
    "        \n",
    "        file_prefix = csv_files[0][:unders[-1]+1]\n",
    "        num_files = len(csv_files)\n",
    "\n",
    "        tif_files = [file_prefix + str(i+1) + '.tif' for i in range(num_files)]  \n",
    "        geojson_files = [file_prefix + str(i+1) + '.geojson' for i in range(num_files)]  \n",
    "\n",
    "        if target_resolution is not None:\n",
    "            scaled_size = int(size * target_resolution / res)\n",
    "            print(f'Pictures from {country} will have {scaled_size}x{scaled_size} pixels.')\n",
    "            scaled_width, scaled_height = scaled_size, scaled_size\n",
    "        else:\n",
    "            scaled_height, scaled_width = height, width\n",
    "        \n",
    "        # iterate over files\n",
    "        for i, (tif, geojson) in enumerate(zip(tif_files, geojson_files)):        \n",
    "\n",
    "            if (i+1) / num_files > train_ratio and not switched_already: \n",
    "                print('Switching to mode val after {} of {} files due to train ratio'.format(\n",
    "                      i, num_files, train_ratio, train_ratio))\n",
    "        \n",
    "                print(base_path, country, curr_path)\n",
    "                export_dir = curr_path\n",
    "                label_field = \"ground_truth\"  \n",
    "\n",
    "                # Export training dataset\n",
    "                if len(curr_dataset) > 0:\n",
    "                    curr_dataset.export(\n",
    "                        export_dir=export_dir,\n",
    "                        dataset_type=fo.types.COCODetectionDataset,\n",
    "                        label_field='ground_truth',\n",
    "                        )\n",
    "                else:\n",
    "                    print(f'Did not export empty dataset for training in {country}')\n",
    "                \n",
    "                curr_path = val_path\n",
    "                curr_dataset = dataset_val\n",
    "                switched_already = True\n",
    "\n",
    "\n",
    "            print(\"Opening geojson file: \", geojson)\n",
    "            # open files and get bands\n",
    "            try:\n",
    "                annots = gpd.read_file(geojson)\n",
    "            except:\n",
    "                print(\"Unable to read annotation file {}\".format(geojson))\n",
    "                print(\"Continuing to the next file...\")\n",
    "                continue\n",
    "\n",
    "            # make sure geojson contains information\n",
    "            if len(annots.columns) == 1:\n",
    "                print('Bad geojson detected! Continuing...') \n",
    "                continue\n",
    "\n",
    "            ds = gdal.Open(tif)\n",
    "            bands = [ds.GetRasterBand(i) for i in range(1, 4)]\n",
    "            info = gdal.Info(tif, format=\"json\")\n",
    "\n",
    "            pd.set_option('display.max_columns', None)\n",
    "\n",
    "            # remove all assets except towers            \n",
    "            remove_assets = [\"DL\", \"TL\", \"OL\", \"SS\"]\n",
    "            for to_remove in remove_assets:\n",
    "                annots = annots[annots[\"label\"] != to_remove]\n",
    "\n",
    "            def to_pixels(geom):\n",
    "                '''\n",
    "                receives pixel coordinates as string and returns columns\n",
    "                upper left, lower right and geometry as Polygon (rectangular)\n",
    "                all coordinates are relative to the tif file the assets is in\n",
    "                '''\n",
    "                geom = geom.split(\" \")\n",
    "                geom.remove('[')\n",
    "                geom.remove(']')\n",
    "\n",
    "                # transform to Polygon with rectangular bbox\n",
    "                geom = [entry for entry in geom if not '[' in entry and not ']' in entry]\n",
    "                geom = [int(float(entry.replace(\",\", \"\"))) for entry in geom]\n",
    "                x, y = geom[::2], geom[1::2] \n",
    "                geom = Polygon([[max(x), max(y)], [max(x), min(y)], [min(x), min(y)], [min(x), max(y)]])\n",
    "                return np.array([min(x), min(y)]), np.array([max(x), max(y)]), geom\n",
    "\n",
    "            # make sure the dataframe contains only towers\n",
    "            annots = annots[annots['geometry'].apply(lambda x: isinstance(x, Polygon))]\n",
    "            if annots.empty: continue\n",
    "\n",
    "            annots[\"ul\"], annots[\"lr\"], annots['geometry'] = zip(*annots['pixel_coordinates'].map(to_pixels))\n",
    "\n",
    "            tif_width, tif_height = info['size'][0], info['size'][1]\n",
    "\n",
    "            for curr, tower in annots.iterrows():\n",
    "\n",
    "                if not isinstance(tower.geometry, Polygon): continue\n",
    "                \n",
    "                if not tower['label'] in tower_types:\n",
    "                    continue\n",
    "\n",
    "                example_name = prefix + '_' + str(np.random.randint(1e10, 1e11)) + '.png'\n",
    "\n",
    "                # define the bounds of random offset\n",
    "                bb_ul, bb_lr = tower['ul'], tower['lr']\n",
    "                min_x, max_x = max(0, bb_lr[0] - scaled_width), min(bb_ul[0], tif_width - scaled_width)\n",
    "                min_y, max_y = max(0, bb_lr[1] - scaled_height), min(bb_ul[1], tif_height - scaled_height)\n",
    "\n",
    "                # randomly draw corner of image (this can fail if towers are close to the frame -> skip tower)\n",
    "                try:\n",
    "                    img_ul_x = np.random.randint(min_x, max_x)\n",
    "                    img_ul_y = np.random.randint(min_y, max_y)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # determine bounding box relative to new image\n",
    "                bb_ul -= np.array([img_ul_x, img_ul_y])\n",
    "                bb_lr -= np.array([img_ul_x, img_ul_y])\n",
    "\n",
    "                # add main tower in image \n",
    "                outer_bbox = [bb_ul[0], bb_ul[1], bb_lr[0]-bb_ul[0], bb_lr[1]-bb_ul[1]]\n",
    "                outer_bbox = (np.array(outer_bbox) / scaled_width).tolist()\n",
    "\n",
    "                if outer_bbox[2] < bbox_threshold or outer_bbox[3] < bbox_threshold:\n",
    "                    continue\n",
    "\n",
    "                # set up image and new filename\n",
    "                new_img = np.zeros((scaled_height, scaled_width, 3), dtype=np.uint8)\n",
    "        \n",
    "                # transfer pixel data\n",
    "                try:\n",
    "                    for i in range(3):\n",
    "                        new_img[:,:,i] = bands[i].ReadAsArray(img_ul_x, img_ul_y, scaled_width, scaled_height)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                # transform array to image\n",
    "                new_img = downsample(new_img, target_size=(size, size))\n",
    "\n",
    "                img = Image.fromarray(new_img, 'RGB')\n",
    "                img.save(os.path.join('./../', curr_path, example_name), quality=100)\n",
    "\n",
    "                # add to dataset\n",
    "                sample = fo.Sample(filepath=os.path.join(\n",
    "                                   base_path, country, curr_path, example_name)\n",
    "                                   )\n",
    "                \n",
    "                \n",
    "                detections = []\n",
    "                detections.append(fo.Detection(label=label, bounding_box=outer_bbox))\n",
    "\n",
    "                # create Polygon of created image\n",
    "                img_corner = np.array([img_ul_x, img_ul_y])\n",
    "                img_polygon = Polygon([\n",
    "                                    img_corner,\n",
    "                                    img_corner + np.array([scaled_width, 0]),\n",
    "                                    img_corner + np.array([scaled_width, scaled_height]),\n",
    "                                    img_corner + np.array([0, scaled_height])\n",
    "                                    ])\n",
    "\n",
    "                # add secondary towers that happen to be in the same image\n",
    "                for j, other in annots.iterrows():\n",
    "\n",
    "                    if tower['geometry'] == other['geometry']:\n",
    "                        continue\n",
    "\n",
    "                    if not other['label'] in tower_types:\n",
    "                        continue\n",
    "\n",
    "                    #if img_polygon.contains(other[\"geometry\"]):\n",
    "                    if img_polygon.intersects(other[\"geometry\"]):\n",
    "\n",
    "                        ul_pixels = np.min(other['geometry'].exterior.xy, axis=1)\n",
    "                        lr_pixels = np.max(other['geometry'].exterior.xy, axis=1)\n",
    "\n",
    "                        ul = (ul_pixels - img_corner) / scaled_width\n",
    "                        lr = (lr_pixels - img_corner) / scaled_width\n",
    "                        w, h = lr - ul\n",
    "\n",
    "                        bbox = [ul[0], ul[1], w, h]\n",
    "\n",
    "                        if not img_polygon.contains(other['geometry']):\n",
    "                            in_part = other['geometry'].intersection(img_polygon)\n",
    "                            shared_fraction = in_part.area / other['geometry'].area\n",
    "\n",
    "                            bbox[0] = max(bbox[0], 0)\n",
    "                            bbox[1] = max(bbox[1], 0)\n",
    "                            bbox[2] = min(bbox[2], 1 - bbox[0])\n",
    "                            bbox[3] = min(bbox[3], 1 - bbox[1])\n",
    "                        \n",
    "                        else:\n",
    "                            shared_fraction = 1\n",
    "\n",
    "                        if bbox[2] < bbox_threshold or bbox[3] < bbox_threshold:\n",
    "                            continue\n",
    "\n",
    "                        if not bbox == outer_bbox and shared_fraction > 0.5:\n",
    "                            detections.append(fo.Detection(label=label, bounding_box=bbox))\n",
    "                \n",
    "                sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "                curr_dataset.add_sample(sample)\n",
    "                \n",
    "                \n",
    "        export_dir = curr_path\n",
    "        label_field = \"ground_truth\"  \n",
    "\n",
    "        # Export training dataset\n",
    "        try:\n",
    "            curr_dataset.export(\n",
    "                    export_dir=export_dir,\n",
    "                    dataset_type=fo.types.COCODetectionDataset,\n",
    "                    label_field='ground_truth',\n",
    "                    )\n",
    "        except ValueError:\n",
    "            print(f'Could not export: {export_dir}; Length: {len(curr_dataset)}')\n",
    "            print('Continuing...')\n",
    "            \n",
    "        fix_filenames(os.path.join(base_path, country, out_val, 'labels.json'))\n",
    "\n",
    "        os.chdir(os.path.abspath(os.path.join('', '../..')))\n",
    "\n",
    "    print('Done with all regions. Proceeding to packaging...')\n",
    "    ds_name = target_base_dir.split('/')[-1]\n",
    "\n",
    "    for mode in ['train', 'val']:\n",
    "\n",
    "        datasets = []\n",
    "        export_dir = os.path.join(target_base_dir, ds_name+'_'+mode)\n",
    "\n",
    "        for country in dirs:\n",
    "\n",
    "            ds_path = os.path.join(target_base_dir, country, mode)\n",
    "\n",
    "            if not os.path.isdir(ds_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                datasets.append(fo.Dataset.from_dir(\n",
    "                        dataset_type=fo.types.COCODetectionDataset,\n",
    "                            data_path=os.path.join(ds_path, 'data'),\n",
    "                            labels_path=os.path.join(ds_path, 'labels.json')))\n",
    "            except ValueError:\n",
    "                continue \n",
    "\n",
    "        if len(datasets) == 0:\n",
    "            print(f'Could not create dataset in mode {mode} for {ds_name}')\n",
    "            continue\n",
    "         \n",
    "        ds = datasets[0]\n",
    "        for curr_ds in datasets[1:]:\n",
    "            ds.merge_samples(curr_ds) \n",
    "\n",
    "        print('Exporting...')\n",
    "        ds.export(\n",
    "                    export_dir=export_dir,\n",
    "                    dataset_type=fo.types.COCODetectionDataset,\n",
    "                    label_field='ground_truth',\n",
    "                    )\n",
    "        print(f'Saved dataset to {export_dir}')\n",
    "\n",
    "        fix_annots(os.path.join(export_dir, 'labels.json'))\n",
    "\n",
    "    print('Done with merging dataset - only cleanup remaining')\n",
    "\n",
    "    for country in dirs:\n",
    "        \n",
    "        to_delete = os.path.join(target_base_dir, country)\n",
    "        if os.path.isdir(to_delete):\n",
    "            shutil.rmtree(to_delete)\n",
    "\n",
    "    print('Prepared datasets!')\n",
    "    train_path = os.path.join(target_base_dir, ds_name+'_train')\n",
    "    val_path = os.path.join(target_base_dir, ds_name+'_val')\n",
    "    print(f'Find training set at: {train_path}')\n",
    "    print(f'Find validation set at: {val_path}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 45824,
     "status": "error",
     "timestamp": 1673960945090,
     "user": {
      "displayName": "Dinah R.",
      "userId": "01454549116054972312"
     },
     "user_tz": -60
    },
    "id": "nBI5bir6f1UC",
    "outputId": "163b3bbf-fd91-48d4-f41b-d69b68b595cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset extraction\n",
      "Note currently all towers are labelled as tower\n",
      "Extracting images from rotorua...\n",
      "Assuming resolution 0.1254966590856217 m/pixel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_right: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_right: invalid type 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pictures from rotorua will have 1427x1427 pixels.\n",
      "Opening geojson file:  NZ_Rotorua_1.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_right: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_right: invalid type 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening geojson file:  NZ_Rotorua_2.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_right: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_right: invalid type 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening geojson file:  NZ_Rotorua_3.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_right: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_right: invalid type 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening geojson file:  NZ_Rotorua_4.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_right: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_right: invalid type 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening geojson file:  NZ_Rotorua_5.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_right: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_right: invalid type 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening geojson file:  NZ_Rotorua_6.geojson\n",
      "Switching to mode val after 6 of 8 files due to train ratio\n",
      "/content/drive/MyDrive/hertie_master_projects/raw_data/duke rotorua /content/drive/MyDrive/hertie_master_projects/testtest/rotorua/train\n",
      "Directory '/content/drive/MyDrive/hertie_master_projects/testtest/rotorua/train' already exists; export will be merged with existing files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiftyone.core.collections:Directory '/content/drive/MyDrive/hertie_master_projects/testtest/rotorua/train' already exists; export will be merged with existing files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 193/193 [5.0s elapsed, 0s remaining, 34.6 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |█████████████████| 193/193 [5.0s elapsed, 0s remaining, 34.6 samples/s]      \n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_right: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_right: invalid type 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening geojson file:  NZ_Rotorua_7.geojson\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_left: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_right: invalid type 3\n",
      "WARNING:fiona.ogrext:Skipping field image_geocoordinates_lower_right: invalid type 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening geojson file:  NZ_Rotorua_8.geojson\n",
      "Directory '/content/drive/MyDrive/hertie_master_projects/testtest/rotorua/val' already exists; export will be merged with existing files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:fiftyone.core.collections:Directory '/content/drive/MyDrive/hertie_master_projects/testtest/rotorua/val' already exists; export will be merged with existing files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 61/61 [1.6s elapsed, 0s remaining, 38.7 samples/s]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |███████████████████| 61/61 [1.6s elapsed, 0s remaining, 38.7 samples/s]         \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-dca21a6bfbdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m              \u001b[0;31m#'brazil',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             ]\n\u001b[0;32m---> 28\u001b[0;31m     extract_duke_dataset(dirs, \n\u001b[0m\u001b[1;32m     29\u001b[0m                          \u001b[0mtarget_base_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                          \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-67ac5feea190>\u001b[0m in \u001b[0;36mextract_duke_dataset\u001b[0;34m(dirs, target_base_dir, size, base_path, train_ratio, target_resolution, bbox_threshold, tower_types)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Continuing...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mfix_filenames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcountry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/hertie_master_projects/utils/dataset_utils.py\u001b[0m in \u001b[0;36mfix_filenames\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     20\u001b[0m     '''\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/hertie_master_projects/raw_data/duke/rotorua/val/labels.json'"
     ]
    }
   ],
   "source": [
    "# ich habe einen Versuch gemacht mit einem testtest folder auf der Ebene des Drive Folders (siehe target_base_dir)\n",
    "# es läuft für mich durch, hat aber bei einigen Files wohl Probleme\n",
    "#WARNING:fiona.ogrext:Skipping field image_geocoordinates_upper_left: invalid type 3\n",
    "# die habe ich mir noch nicht genauer angeguckt\n",
    "\n",
    "# und dann gibt er mir momentan noch den Fehler, dass er '/content/drive/MyDrive/hertie_master_projects/raw_data/duke/rotorua/val/labels.json' nicht findet, obwohl der file eigentlich da ist.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"/content/drive/MyDrive/hertie_master_projects/raw_data/duke\"\n",
    "    assert not '.' in base_path, (\"This is the placeholder path, please\"\n",
    "                                  \" configure base_path to the duke data\")\n",
    "\n",
    "    target_base_dir = \"/content/drive/MyDrive/hertie_master_projects/testtest\"\n",
    "    assert not '.' in target_base_dir, (\"This is the placeholder path, please\"\n",
    "                                        \" configure target_base_dir to the dataset path\")\n",
    "    dataset_name = 'testset'\n",
    "\n",
    "    os.chdir(base_path)\n",
    "    dirs = [ \n",
    "            #'hartford',   #  (APPEARS TO HAVE CORRUPTED GEOJSON FILES)\n",
    "            # 'china',\n",
    "             #'kansas',\n",
    "             #'dunedin',\n",
    "             #'gisborne',\n",
    "             #'palmertson',\n",
    "             'rotorua'#,\n",
    "             #'tauranga',\n",
    "             #'wilmington',\n",
    "             #'arizona',\n",
    "             #'clyde',\n",
    "             #'sudan',\n",
    "             #'mexico',\n",
    "             #'brazil',\n",
    "            ]\n",
    "    extract_duke_dataset(dirs, \n",
    "                         target_base_dir,\n",
    "                         size=512, \n",
    "                         train_ratio=0.8,\n",
    "                         base_path=base_path,\n",
    "                         target_resolution=0.35,\n",
    "                         #out_train=trainset_name,\n",
    "                         tower_types=[\"TT\", \"DT\", \"OT\"] \n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwYkrPExnifk"
   },
   "source": [
    "# Neuer Abschnitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmBOYkUAkzxa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
